{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     diagnosis  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "..         ...  \n",
       "564          0  \n",
       "565          0  \n",
       "566          0  \n",
       "567          0  \n",
       "568          1  \n",
       "\n",
       "[569 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('Breast_cancer_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1857749  0.11949647 0.30542971 0.28018765 0.10911128]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x240977f9e48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df[['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness']]\n",
    "Y = df[['diagnosis']]\n",
    "model = RandomForestClassifier(n_estimators=340)\n",
    "model.fit(X,Y)\n",
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "importance_df = pd.DataFrame({\"Features\":pd.DataFrame(X).columns,\"Importances\":feature_importances})\n",
    "importance_df.set_index(\"Importances\")\n",
    "importance_df = importance_df.sort_values(\"Importances\")\n",
    "importance_df.plot.bar(color=\"teal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 3) (171, 3)\n",
      "(398, 1) (171, 1)\n"
     ]
    }
   ],
   "source": [
    "x_new = df[['mean_radius','mean_perimeter','mean_area']]\n",
    "y_new = df[['diagnosis']]\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_new,y_new,random_state=42,test_size=0.3)\n",
    "print(x_train.shape,x_test.shape)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "linear = LogisticRegression()\n",
    "linear.fit(x_train,y_train)\n",
    "y_pred = linear.predict(x_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9239766081871345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=85, random_state=0)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "398/398 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.6407\n",
      "Epoch 2/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.5067 - accuracy: 0.8643\n",
      "Epoch 3/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.4067 - accuracy: 0.8643\n",
      "Epoch 4/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.3473 - accuracy: 0.8693\n",
      "Epoch 5/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.3227 - accuracy: 0.8693\n",
      "Epoch 6/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.3124 - accuracy: 0.8693\n",
      "Epoch 7/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.3088 - accuracy: 0.8693\n",
      "Epoch 8/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.3070 - accuracy: 0.8693\n",
      "Epoch 9/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.3063 - accuracy: 0.8693\n",
      "Epoch 10/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.3057 - accuracy: 0.8719\n",
      "Epoch 11/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.3046 - accuracy: 0.8693\n",
      "Epoch 12/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.3036 - accuracy: 0.8668\n",
      "Epoch 13/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.3033 - accuracy: 0.8693\n",
      "Epoch 14/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.3019 - accuracy: 0.8693\n",
      "Epoch 15/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.3025 - accuracy: 0.8668\n",
      "Epoch 16/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.3014 - accuracy: 0.8668\n",
      "Epoch 17/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.3000 - accuracy: 0.8693\n",
      "Epoch 18/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2994 - accuracy: 0.8693\n",
      "Epoch 19/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2983 - accuracy: 0.8668\n",
      "Epoch 20/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2978 - accuracy: 0.8719\n",
      "Epoch 21/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2968 - accuracy: 0.8719\n",
      "Epoch 22/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2960 - accuracy: 0.8719\n",
      "Epoch 23/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2964 - accuracy: 0.8744\n",
      "Epoch 24/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2945 - accuracy: 0.8719\n",
      "Epoch 25/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2953 - accuracy: 0.8719\n",
      "Epoch 26/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2936 - accuracy: 0.8693\n",
      "Epoch 27/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2919 - accuracy: 0.8693\n",
      "Epoch 28/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.2914 - accuracy: 0.8668\n",
      "Epoch 29/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.2900 - accuracy: 0.8719\n",
      "Epoch 30/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2882 - accuracy: 0.8744\n",
      "Epoch 31/256\n",
      "398/398 [==============================] - 0s 88us/step - loss: 0.2877 - accuracy: 0.8719\n",
      "Epoch 32/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2879 - accuracy: 0.8744\n",
      "Epoch 33/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2849 - accuracy: 0.8769\n",
      "Epoch 34/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2856 - accuracy: 0.8794\n",
      "Epoch 35/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2836 - accuracy: 0.8769\n",
      "Epoch 36/256\n",
      "398/398 [==============================] - 0s 80us/step - loss: 0.2823 - accuracy: 0.8794\n",
      "Epoch 37/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2802 - accuracy: 0.8819\n",
      "Epoch 38/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2802 - accuracy: 0.8844\n",
      "Epoch 39/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2786 - accuracy: 0.8794\n",
      "Epoch 40/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2764 - accuracy: 0.8819\n",
      "Epoch 41/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2768 - accuracy: 0.8844\n",
      "Epoch 42/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2768 - accuracy: 0.8844\n",
      "Epoch 43/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2748 - accuracy: 0.8819\n",
      "Epoch 44/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2749 - accuracy: 0.8819\n",
      "Epoch 45/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2763 - accuracy: 0.8869\n",
      "Epoch 46/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2724 - accuracy: 0.8869\n",
      "Epoch 47/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.2692 - accuracy: 0.8869\n",
      "Epoch 48/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2684 - accuracy: 0.8869\n",
      "Epoch 49/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2670 - accuracy: 0.8819\n",
      "Epoch 50/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2702 - accuracy: 0.8794\n",
      "Epoch 51/256\n",
      "398/398 [==============================] - 0s 80us/step - loss: 0.2671 - accuracy: 0.8869\n",
      "Epoch 52/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2640 - accuracy: 0.8869\n",
      "Epoch 53/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2636 - accuracy: 0.8819\n",
      "Epoch 54/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2627 - accuracy: 0.8894\n",
      "Epoch 55/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2648 - accuracy: 0.8844\n",
      "Epoch 56/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2644 - accuracy: 0.8819\n",
      "Epoch 57/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2581 - accuracy: 0.8869\n",
      "Epoch 58/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2638 - accuracy: 0.8819\n",
      "Epoch 59/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2570 - accuracy: 0.8869\n",
      "Epoch 60/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2591 - accuracy: 0.8844\n",
      "Epoch 61/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2563 - accuracy: 0.8844\n",
      "Epoch 62/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2632 - accuracy: 0.8894\n",
      "Epoch 63/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2532 - accuracy: 0.8869\n",
      "Epoch 64/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2558 - accuracy: 0.8869\n",
      "Epoch 65/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2667 - accuracy: 0.8894\n",
      "Epoch 66/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2574 - accuracy: 0.8869\n",
      "Epoch 67/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2587 - accuracy: 0.8844\n",
      "Epoch 68/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2511 - accuracy: 0.8844\n",
      "Epoch 69/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2525 - accuracy: 0.8844\n",
      "Epoch 70/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2483 - accuracy: 0.8894\n",
      "Epoch 71/256\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.2519 - accuracy: 0.8920\n",
      "Epoch 72/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2486 - accuracy: 0.8869\n",
      "Epoch 73/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2486 - accuracy: 0.8894\n",
      "Epoch 74/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2466 - accuracy: 0.8945\n",
      "Epoch 75/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2477 - accuracy: 0.8894\n",
      "Epoch 76/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2475 - accuracy: 0.8945\n",
      "Epoch 77/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2471 - accuracy: 0.8920\n",
      "Epoch 78/256\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.2453 - accuracy: 0.8920\n",
      "Epoch 79/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2462 - accuracy: 0.8970\n",
      "Epoch 80/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 65us/step - loss: 0.2460 - accuracy: 0.8995\n",
      "Epoch 81/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2478 - accuracy: 0.9020\n",
      "Epoch 82/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2486 - accuracy: 0.8945\n",
      "Epoch 83/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2448 - accuracy: 0.8920\n",
      "Epoch 84/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2435 - accuracy: 0.9020\n",
      "Epoch 85/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2438 - accuracy: 0.8995\n",
      "Epoch 86/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2428 - accuracy: 0.8995\n",
      "Epoch 87/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2429 - accuracy: 0.8945\n",
      "Epoch 88/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2422 - accuracy: 0.8970\n",
      "Epoch 89/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2431 - accuracy: 0.8945\n",
      "Epoch 90/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2442 - accuracy: 0.8945\n",
      "Epoch 91/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2411 - accuracy: 0.8970\n",
      "Epoch 92/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2408 - accuracy: 0.8995\n",
      "Epoch 93/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2481 - accuracy: 0.8920\n",
      "Epoch 94/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2491 - accuracy: 0.9020\n",
      "Epoch 95/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2410 - accuracy: 0.9020\n",
      "Epoch 96/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2406 - accuracy: 0.8970\n",
      "Epoch 97/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2395 - accuracy: 0.8970\n",
      "Epoch 98/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2403 - accuracy: 0.8970\n",
      "Epoch 99/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2424 - accuracy: 0.9045\n",
      "Epoch 100/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2406 - accuracy: 0.8970\n",
      "Epoch 101/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2410 - accuracy: 0.9045\n",
      "Epoch 102/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2390 - accuracy: 0.8995\n",
      "Epoch 103/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2395 - accuracy: 0.8995\n",
      "Epoch 104/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2445 - accuracy: 0.8995\n",
      "Epoch 105/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2391 - accuracy: 0.9020\n",
      "Epoch 106/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2417 - accuracy: 0.9045\n",
      "Epoch 107/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2364 - accuracy: 0.9070\n",
      "Epoch 108/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2421 - accuracy: 0.8995\n",
      "Epoch 109/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2411 - accuracy: 0.9070\n",
      "Epoch 110/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2444 - accuracy: 0.8995\n",
      "Epoch 111/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2370 - accuracy: 0.9070\n",
      "Epoch 112/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2388 - accuracy: 0.9020\n",
      "Epoch 113/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2363 - accuracy: 0.9020\n",
      "Epoch 114/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2357 - accuracy: 0.9020\n",
      "Epoch 115/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2362 - accuracy: 0.8995\n",
      "Epoch 116/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2384 - accuracy: 0.9045\n",
      "Epoch 117/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2378 - accuracy: 0.8995\n",
      "Epoch 118/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2372 - accuracy: 0.9045\n",
      "Epoch 119/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2363 - accuracy: 0.9045\n",
      "Epoch 120/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2403 - accuracy: 0.9045\n",
      "Epoch 121/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2412 - accuracy: 0.8995\n",
      "Epoch 122/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2358 - accuracy: 0.9045\n",
      "Epoch 123/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2376 - accuracy: 0.8995\n",
      "Epoch 124/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2372 - accuracy: 0.9020\n",
      "Epoch 125/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2374 - accuracy: 0.8995\n",
      "Epoch 126/256\n",
      "398/398 [==============================] - 0s 50us/step - loss: 0.2387 - accuracy: 0.9045\n",
      "Epoch 127/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2328 - accuracy: 0.8995\n",
      "Epoch 128/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2370 - accuracy: 0.9045\n",
      "Epoch 129/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2343 - accuracy: 0.9045\n",
      "Epoch 130/256\n",
      "398/398 [==============================] - 0s 50us/step - loss: 0.2340 - accuracy: 0.9070\n",
      "Epoch 131/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2366 - accuracy: 0.9045\n",
      "Epoch 132/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2339 - accuracy: 0.9020\n",
      "Epoch 133/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2382 - accuracy: 0.9045\n",
      "Epoch 134/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2329 - accuracy: 0.9020\n",
      "Epoch 135/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2352 - accuracy: 0.9070\n",
      "Epoch 136/256\n",
      "398/398 [==============================] - 0s 50us/step - loss: 0.2362 - accuracy: 0.9020\n",
      "Epoch 137/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2379 - accuracy: 0.9070\n",
      "Epoch 138/256\n",
      "398/398 [==============================] - 0s 50us/step - loss: 0.2330 - accuracy: 0.9095\n",
      "Epoch 139/256\n",
      "398/398 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.93 - 0s 55us/step - loss: 0.2335 - accuracy: 0.9045\n",
      "Epoch 140/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2346 - accuracy: 0.9045\n",
      "Epoch 141/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2327 - accuracy: 0.9045\n",
      "Epoch 142/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2351 - accuracy: 0.8995\n",
      "Epoch 143/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2319 - accuracy: 0.9070\n",
      "Epoch 144/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2365 - accuracy: 0.8995\n",
      "Epoch 145/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2357 - accuracy: 0.9070\n",
      "Epoch 146/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2333 - accuracy: 0.9045\n",
      "Epoch 147/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2312 - accuracy: 0.9095\n",
      "Epoch 148/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2326 - accuracy: 0.9045\n",
      "Epoch 149/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2332 - accuracy: 0.9045\n",
      "Epoch 150/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2313 - accuracy: 0.9095\n",
      "Epoch 151/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2363 - accuracy: 0.9045\n",
      "Epoch 152/256\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2312 - accuracy: 0.9020\n",
      "Epoch 153/256\n",
      "398/398 [==============================] - 0s 83us/step - loss: 0.2355 - accuracy: 0.8995\n",
      "Epoch 154/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2376 - accuracy: 0.9070\n",
      "Epoch 155/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2325 - accuracy: 0.9045\n",
      "Epoch 156/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2310 - accuracy: 0.9045\n",
      "Epoch 157/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2353 - accuracy: 0.9045\n",
      "Epoch 158/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 68us/step - loss: 0.2295 - accuracy: 0.9045\n",
      "Epoch 159/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2345 - accuracy: 0.9020\n",
      "Epoch 160/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2301 - accuracy: 0.9095\n",
      "Epoch 161/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2333 - accuracy: 0.8995\n",
      "Epoch 162/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2301 - accuracy: 0.9070\n",
      "Epoch 163/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2321 - accuracy: 0.9070\n",
      "Epoch 164/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2329 - accuracy: 0.8970\n",
      "Epoch 165/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2352 - accuracy: 0.9070\n",
      "Epoch 166/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2367 - accuracy: 0.8995\n",
      "Epoch 167/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2315 - accuracy: 0.9020\n",
      "Epoch 168/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2294 - accuracy: 0.9070\n",
      "Epoch 169/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2312 - accuracy: 0.9070\n",
      "Epoch 170/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2321 - accuracy: 0.9045\n",
      "Epoch 171/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2286 - accuracy: 0.9095\n",
      "Epoch 172/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2346 - accuracy: 0.8970\n",
      "Epoch 173/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2358 - accuracy: 0.9045\n",
      "Epoch 174/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2298 - accuracy: 0.9045\n",
      "Epoch 175/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2332 - accuracy: 0.9020\n",
      "Epoch 176/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2321 - accuracy: 0.9045\n",
      "Epoch 177/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2281 - accuracy: 0.9070\n",
      "Epoch 178/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2316 - accuracy: 0.9020\n",
      "Epoch 179/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2274 - accuracy: 0.9070\n",
      "Epoch 180/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2297 - accuracy: 0.9045\n",
      "Epoch 181/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2285 - accuracy: 0.9045\n",
      "Epoch 182/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2291 - accuracy: 0.9095\n",
      "Epoch 183/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2273 - accuracy: 0.9095\n",
      "Epoch 184/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2278 - accuracy: 0.9121\n",
      "Epoch 185/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2329 - accuracy: 0.9020\n",
      "Epoch 186/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2285 - accuracy: 0.9095\n",
      "Epoch 187/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2284 - accuracy: 0.9070\n",
      "Epoch 188/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2274 - accuracy: 0.9045\n",
      "Epoch 189/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2278 - accuracy: 0.9121\n",
      "Epoch 190/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2294 - accuracy: 0.9045\n",
      "Epoch 191/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2300 - accuracy: 0.9020\n",
      "Epoch 192/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2280 - accuracy: 0.9045\n",
      "Epoch 193/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2278 - accuracy: 0.9070\n",
      "Epoch 194/256\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.2291 - accuracy: 0.9070\n",
      "Epoch 195/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2286 - accuracy: 0.9070\n",
      "Epoch 196/256\n",
      "398/398 [==============================] - 0s 53us/step - loss: 0.2276 - accuracy: 0.9095\n",
      "Epoch 197/256\n",
      "398/398 [==============================] - 0s 50us/step - loss: 0.2314 - accuracy: 0.9070\n",
      "Epoch 198/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2259 - accuracy: 0.9121\n",
      "Epoch 199/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2276 - accuracy: 0.9095\n",
      "Epoch 200/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2258 - accuracy: 0.9070\n",
      "Epoch 201/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2277 - accuracy: 0.9045\n",
      "Epoch 202/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2292 - accuracy: 0.9070\n",
      "Epoch 203/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2292 - accuracy: 0.8970\n",
      "Epoch 204/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2273 - accuracy: 0.9045\n",
      "Epoch 205/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2245 - accuracy: 0.9045\n",
      "Epoch 206/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2294 - accuracy: 0.9070\n",
      "Epoch 207/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2300 - accuracy: 0.9020\n",
      "Epoch 208/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2265 - accuracy: 0.8995\n",
      "Epoch 209/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2254 - accuracy: 0.9095\n",
      "Epoch 210/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2306 - accuracy: 0.9020\n",
      "Epoch 211/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2241 - accuracy: 0.9020\n",
      "Epoch 212/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2296 - accuracy: 0.9045\n",
      "Epoch 213/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2358 - accuracy: 0.8995\n",
      "Epoch 214/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2255 - accuracy: 0.9020\n",
      "Epoch 215/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2261 - accuracy: 0.9045\n",
      "Epoch 216/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2268 - accuracy: 0.8995\n",
      "Epoch 217/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2279 - accuracy: 0.9045\n",
      "Epoch 218/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2283 - accuracy: 0.8995\n",
      "Epoch 219/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2221 - accuracy: 0.9095\n",
      "Epoch 220/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2257 - accuracy: 0.9146\n",
      "Epoch 221/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2285 - accuracy: 0.8995\n",
      "Epoch 222/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2234 - accuracy: 0.9045\n",
      "Epoch 223/256\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.2283 - accuracy: 0.9070\n",
      "Epoch 224/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2261 - accuracy: 0.9070\n",
      "Epoch 225/256\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2267 - accuracy: 0.9070\n",
      "Epoch 226/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2231 - accuracy: 0.9095\n",
      "Epoch 227/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2231 - accuracy: 0.9095\n",
      "Epoch 228/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2233 - accuracy: 0.9045\n",
      "Epoch 229/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2250 - accuracy: 0.9070\n",
      "Epoch 230/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2247 - accuracy: 0.9121\n",
      "Epoch 231/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2224 - accuracy: 0.8995\n",
      "Epoch 232/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2242 - accuracy: 0.9095\n",
      "Epoch 233/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2308 - accuracy: 0.9070\n",
      "Epoch 234/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2283 - accuracy: 0.8995\n",
      "Epoch 235/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2231 - accuracy: 0.8970\n",
      "Epoch 236/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 68us/step - loss: 0.2245 - accuracy: 0.9020\n",
      "Epoch 237/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2216 - accuracy: 0.9121\n",
      "Epoch 238/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2216 - accuracy: 0.9045\n",
      "Epoch 239/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2213 - accuracy: 0.9095\n",
      "Epoch 240/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2212 - accuracy: 0.9070\n",
      "Epoch 241/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2270 - accuracy: 0.9095\n",
      "Epoch 242/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2221 - accuracy: 0.8995\n",
      "Epoch 243/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2224 - accuracy: 0.9070\n",
      "Epoch 244/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2217 - accuracy: 0.9045\n",
      "Epoch 245/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2215 - accuracy: 0.9121\n",
      "Epoch 246/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2208 - accuracy: 0.9070\n",
      "Epoch 247/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2223 - accuracy: 0.9121\n",
      "Epoch 248/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2198 - accuracy: 0.9070\n",
      "Epoch 249/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2266 - accuracy: 0.9095\n",
      "Epoch 250/256\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.2230 - accuracy: 0.9045\n",
      "Epoch 251/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2202 - accuracy: 0.9095\n",
      "Epoch 252/256\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.2267 - accuracy: 0.8945\n",
      "Epoch 253/256\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2259 - accuracy: 0.9121\n",
      "Epoch 254/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2235 - accuracy: 0.9121\n",
      "Epoch 255/256\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2201 - accuracy: 0.8970\n",
      "Epoch 256/256\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2188 - accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Dense(64, activation='relu',input_shape=(3,)),    \n",
    "                    Dense(64, activation='relu'),\n",
    "                    Dense(1, activation='sigmoid'),])\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train,y_train,batch_size=32, epochs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 201us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15560583953271834, 0.9356725215911865]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_radius = 14.05\n",
    "mean_perimeter = 91.38\n",
    "mean_area = 600.4\n",
    "new_data = np.array([[mean_radius, mean_perimeter, mean_area]])\n",
    "prediction = model.predict(new_data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2409cd5af48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predictions(mean_radius, mean_perimeter, mean_area):\n",
    "    new_data = np.array([[mean_radius, mean_perimeter, mean_area]])\n",
    "    prediction = linear.predict(new_data)\n",
    "    if prediction[0] == 0:\n",
    "        return \"Negative ðŸŽ‰\"\n",
    "    else:\n",
    "        return \"Positive ðŸ˜ž\"\n",
    "\n",
    "def something(hello):\n",
    "    print(\"Hello\" + hello)\n",
    "\n",
    "iface = gr.Interface(\n",
    "  fn=predictions,\n",
    "  inputs=[\"number\", \"number\", \"number\"],\n",
    "  outputs=[ \"text\"])\n",
    "   #interpretation=\"default\"\n",
    "iface.launch('share=True')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
